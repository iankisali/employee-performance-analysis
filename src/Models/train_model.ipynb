{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36099c02-80b6-455a-8659-1385dca0d5ab",
   "metadata": {},
   "source": [
    "## Model Training for Employee Performance Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2874f2c2-f0a1-43a1-a728-01ac635922a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0.tar.gz (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lightgbm) (1.15.2)\n",
      "Building wheels for collected packages: lightgbm\n",
      "  Building wheel for lightgbm (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lightgbm: filename=lightgbm-4.6.0-py3-none-linux_x86_64.whl size=2737776 sha256=eeb03bf869618ac3acd6f7173e1af6ce7688888cec6abd18d56e6eaddeb34cf0\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/bb/db/6d/7814aed03437129dc284a055c084f201b765deb54b6908efab\n",
      "Successfully built lightgbm\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n"
     ]
    }
   ],
   "source": [
    "# Installing module\n",
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d2b072f-7c73-4575-b7bc-2b558b25650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bba07904-26ee-46ac-84ec-880f92fef814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded, processed and split into training and testing sets.\n"
     ]
    }
   ],
   "source": [
    "# Importing data, Encoding and splitting data for the model\n",
    "data_path = \"/home/ec2-user/SageMaker/data/EmployeeData_Raw.csv\"\n",
    "\n",
    "try:\n",
    "    employee_df = pd.read_csv(data_path)\n",
    "    X = employee_df.drop(['PerformanceRating', 'EmpNumber'], axis=1)\n",
    "    y = employee_df['PerformanceRating']\n",
    "\n",
    "    # One hot encoding of categorical features\n",
    "    categorical_features = X.select_dtypes(include='object').columns\n",
    "    X_processed = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n",
    "\n",
    "    # Splitting data into training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    print(\"Data loaded, processed and split into training and testing sets.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {data_path} not found\")\n",
    "    X_train, X_test, y_train, y_test = None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f7f59-af26-46a1-b5de-02593f2f3283",
   "metadata": {},
   "source": [
    "### Algorithm selection\n",
    "\n",
    "Several classification algorithm are used to predict the Performance Rating. The classification tasks include:\n",
    "- Logistic regression\n",
    "- Random Forest Classifier\n",
    "- LightGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1f441ae-21bf-471f-9a7a-85ec0f068ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training logistic regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model trained.\n",
      "Training random forest classifier...\n",
      "Random forest classifier model trained.\n",
      "Training lightGBM classifier...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 394\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score -1.823508\n",
      "[LightGBM] [Info] Start training from score -0.317283\n",
      "[LightGBM] [Info] Start training from score -2.203494\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "lightGBM classifier model trained.\n",
      "\n",
      "All models trained successfully.\n"
     ]
    }
   ],
   "source": [
    "# Model training using selected model\n",
    "logistic_regression = LogisticRegression(max_iter=1000, random_state=42)\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "lgbm_classifier = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "# Training models\n",
    "print(\"Training logistic regression...\")\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "print(\"Logistic regression model trained.\")\n",
    "\n",
    "print(\"Training random forest classifier...\")\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "print(\"Random forest classifier model trained.\")\n",
    "\n",
    "print(\"Training lightGBM classifier...\")\n",
    "lgbm_classifier.fit(X_train, y_train)\n",
    "print(\"lightGBM classifier model trained.\")\n",
    "\n",
    "print(\"\\nAll models trained successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bd2611c-f3b1-4f51-b8fc-21f7d4b36d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest parameters: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Hypaparemeter tuning to get optimal output from the model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid_rf = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "\n",
    "grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=5, scoring='accuracy')\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "best_rf_clf = grid_search_rf.best_estimator_\n",
    "print(f\"Best Random Forest parameters: {grid_search_rf.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b0aee16-e824-4d10-98d7-bc7917b294f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Logistic Regression model.\n",
      "Saved Random Forest model.\n",
      "Saved LightGBM model.\n",
      "Trained models saved to '/home/ec2-user/SageMaker/src/Models' directory.\n"
     ]
    }
   ],
   "source": [
    "# Saving trained models\n",
    "try:\n",
    "    model_dir = '/home/ec2-user/SageMaker/src/Models'\n",
    "    import os\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    joblib.dump(logistic_regression, os.path.join(model_dir, 'logistic_regression_model.joblib'))\n",
    "    print(\"Saved Logistic Regression model.\")\n",
    "\n",
    "    joblib.dump(rf_classifier, os.path.join(model_dir, 'random_forest_model.joblib'))\n",
    "    print(\"Saved Random Forest model.\")\n",
    "\n",
    "    joblib.dump(lgbm_classifier, os.path.join(model_dir, 'lightgbm_model.joblib'))\n",
    "    print(\"Saved LightGBM model.\")\n",
    "\n",
    "    print(f\"Trained models saved to '{model_dir}' directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving models: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
